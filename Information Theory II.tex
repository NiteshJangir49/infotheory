\documentclass[11pt]{article}

%%% XeLaTeX Font Definitions

\usepackage{titlesec}
\usepackage{titling}
\usepackage{xunicode}
\usepackage{fontspec,xltxtra,xunicode}
\usepackage[table,xcdraw]{xcolor}
\defaultfontfeatures{Mapping=tex-text}
\usepackage{bigints}
\usepackage{booktabs}
\usepackage{bm}

% Uncomment below to change default features 
%\setromanfont[Mapping=tex-text]{Hoefler Text}
%\setsansfont[Scale=MatchLowercase,Mapping=tex-text]{Gill Sans}
%\setmonofont[Scale=MatchLowercase]{Andale Mono}

% Specify different font for section headings
\newfontfamily\headingfont[]{Lucida Grande Bold}
\newfontfamily\titlefont[]{Optima}

\titleformat*{\section}{\Large\headingfont}
\titleformat*{\subsection}{\large\headingfont}
\titleformat*{\subsubsection}{\large\headingfont}
\renewcommand{\maketitlehooka}{\titlefont}

%%% Remove the "abstract" word before the abstract

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\usepackage{abstract}
\renewcommand{\abstractname}{}    % clear the title
\renewcommand{\absnamepos}{empty} % originally center

%%% Actual Preamble

%\headheight=8pt
%\topmargin=3pt
%\textheight=624pt
%\textwidth=432pt
%\oddsidemargin=18pt
%\evensidemargin=18pt
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{epsfig}
\usepackage{psfrag}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

%\usepackage{sseq} (if you need to draw spectral sequences, please use this package, available at http://wwwmath.uni-muenster.de/u/tbauer/)
\usepackage{mathrsfs}
\usepackage{amscd}
\usepackage[all]{xy}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{amsbsy}
\usepackage{verbatim}
\usepackage{moreverb}
\usepackage{mathdots}
\usepackage{setspace}
%\usepackage{eucal}
\usepackage{hyperref}
\usepackage{pgfplots}%http://www.ctan.org/pkg/pgfplots

\usepackage{listings}
\usepackage[margin=1in]{geometry}
\pagestyle{plain}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
%\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{notation}{Notation}
\newtheorem{summary}{Summary}
\newtheorem{note}{Note}
\newtheorem{construction}[theorem]{Construction}
%\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{question}[example]{Question}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\coeq}{coeq}
\DeclareMathOperator{\colim}{colim}
\DeclareMathOperator{\cone}{cone}
\DeclareMathOperator{\Der}{Der}
\DeclareMathOperator{\Ext}{Ext}
\DeclareMathOperator{\hocolim}{hocolim}
\DeclareMathOperator{\holim}{holim}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Iso}{Iso}
\DeclareMathOperator{\Map}{Map}
\DeclareMathOperator{\Tot}{Tot}
\DeclareMathOperator{\Tor}{Tor}
\DeclareMathOperator{\Spec}{Spec}
\newcommand{\TMF}{\mathit{TMF}}
\newcommand{\tmf}{\mathit{tmf}}
\newcommand{\Mell}{\mathcal M_{\mathit{ell}}}
\newcommand{\Mord}{\mathcal M_{\mathit{ell}}^{\mathit{ord}}}
\newcommand{\Mss}{\mathcal M_{\mathit{ell}}^{\mathit{ss}}}
\newcommand{\Mbar}{\overline{\mathcal M}_{\mathit{ell}}}
\newcommand{\Mfg}{\mathcal M_{\mathit{FGL}}}
\newcommand{\MU}{\mathit{MU}}
\newcommand{\MP}{\mathit{MP}}
\newcommand{\Lk}{L_{K(n)}}
\newcommand{\Lone}{L_{K(1)}}
\newcommand{\Ltwo}{L_{K(2)}}
\newcommand{\Sp}{\mathbf{Sp}}
\newcommand{\Eoo}{E_\infty}
\newcommand{\Aoo}{A_\infty}
\newcommand{\CP}{\mathbb{CP}^\infty}
\newcommand{\GL}{\mathit{GL}}
\newcommand{\gl}{\mathit{gl}}
\newcommand{\nn}{\nonumber}
\newcommand{\nid}{\noindent}
\newcommand{\ra}{\rightarrow}
\newcommand{\la}{\leftarrow}
\newcommand{\xra}{\xrightarrow}
\newcommand{\xla}{\xleftarrow}
\newcommand{\weq}{\xrightarrow{\sim}}
\newcommand{\cofib}{\rightarrowtail}
\newcommand{\fib}{\twoheadrightarrow}
 \newcommand{\xhdr}[1]{\vspace{2mm}\noindent{{\bf #1.}}}

\def\llarrow{   \hspace{.05cm}\mbox{\,\put(0,-2){$\leftarrow$}\put(0,2){$\leftarrow$}\hspace{.45cm}}}
\def\rrarrow{   \hspace{.05cm}\mbox{\,\put(0,-2){$\rightarrow$}\put(0,2){$\rightarrow$}\hspace{.45cm}}}
\def\lllarrow{  \hspace{.05cm}\mbox{\,\put(0,-3){$\leftarrow$}\put(0,1){$\leftarrow$}\put(0,5){$\leftarrow$}\hspace{.45cm}}}
\def\rrrarrow{  \hspace{.05cm}\mbox{\,\put(0,-3){$\rightarrow$}\put(0,1){$\rightarrow$}\put(0,5){$\rightarrow$}\hspace{.45cm}}}
\def\cA{\mathcal A}\def\cB{\mathcal B}\def\cc{\mathbf C}\def\cd{\mathbf D}
\def\ce{\mathcal E}\def\cf{\mathcal F}\def\cG{\mathcal G}\def\cH{\mathcal H}
\def\cI{\mathcal I}\def\cJ{\mathcal J}\def\cK{\mathcal K}\def\cL{\mathcal L}
\def\cM{\mathbf M}\def\cN{\mathcal N}\def\cO{\mathbf O}\def\cP{\mathcal P}
\def\cQ{\mathcal Q}\def\cR{\mathcal R}\def\cS{\mathcal S}\def\cT{\mathcal T}
\def\cU{\mathcal U}\def\cV{\mathcal V}\def\cW{\mathcal W}\def\cX{\mathcal X}
\def\cY{\mathcal Y}\def\cZ{\mathcal Z}
\def\AA{\mathbb A}\def\BB{\mathbb B}\def\CC{\mathbb C}\def\DD{\mathbb D}
\def\EE{\mathbb E}\def\FF{\mathbb F}\def\GG{\mathbb G}\def\HH{\mathbb H}
\def\II{\mathbb I}\def\JJ{\mathbb J}\def\KK{\mathbb K}\def\LL{\mathbb L}
\def\MM{\mathbb M}\def\NN{\mathbb N}\def\OO{\mathbb O}\def\PP{\mathbb P}
\def\QQ{\mathbb Q}\def\RR{\mathbb R}\def\SS{\mathbb S}\def\TT{\mathbb T}
\def\UU{\mathbb U}\def\VV{\mathbb V}\def\WW{\mathbb W}\def\XX{\mathbb X}
\def\YY{\mathbb Y}\def\ZZ{\mathbb Z}

\newcommand{\MFGL}{\mathcal M_{\mathit{FGL}}}
\newcommand{\calO}{{\mathcal O}}
\newcommand{\calC}{{\mathcal C}}
\newcommand{\set}{{\mathrm{Set}}}
\newcommand{\Deltab}{{\mathbf \Delta}}
\newcommand{\spet}{\mathrm{Spec}^\mathrm{\acute{e}t}}
\newcommand{\Z}{\mathbb Z}
\DeclareMathOperator{\Spf}{Spf}

\usepackage{fancyhdr}
\setlength{\headheight}{15.2pt}
\pagestyle{fancy}

\lhead{2016-17}
\chead{Information Theory}
\rhead{Manan Shah}

\graphicspath{{./figures/}}

\begin{document}
\title{\headingfont{Information Theory, Part II}}
\author{Manan Shah\\ \texttt{manan.shah.777@gmail.com} \\ The Harker School}
\maketitle
\begin{abstract}
This document contains lecture notes from Harker's Advanced Topics in Mathematics class in Information Theory II, taught by Dr. Anuradha Aiyer. This course is the second part of a two part offering that explores the basic concepts of Information Theory, as initially described by Claude Elwood Shannon at Bell Labs in 1948. In Part 2 of the course, we explore other applications of Information Theory to the disciplines of Gambling, Statistics, Physics, Computer science, Economics and Philosophy. These notes were taken using TeXShop and \LaTeX2$\epsilon$ and will be updated for each class. The reader is advised to note any errata at the source control repository \texttt{https://github.com/mananshah99/infotheory}.
\end{abstract}
\tableofcontents
\newpage

%% Notes start here

\section{Unit 1: Gambling}

We'll discuss the duality between the growth rate of investment (i.e. a horse race) and the entropy rate of the horse race and how the side information's financial value is tied to mutual information. 

\definition[Horse Race] We have $m$ horses in a race in which the $i$th horse wins with probability $p_i$. If horse $i$ wins, the payoff is $o_i$ for 1\footnote{There are two ways to describe a bet: either $a$ for 1 or $b$ to 1. The first notation indicates an exchange that happens prior to the race, and the latter indicates and exchange that happens post-race (although in both cases the horses are picked before the race). More concretely, $a$ for 1 indicates that if one places \$1 on a particular horse before the race, the payoff is \$a iff the horse wins and \$0 if the horse loses. $b$ to 1 indicates that one would pay \$1 after the race if a particular horse loses and win \$b if the horse wins. The equivalency between these scenarios is $b = a-1$.}. We'll assume that the gambler invests his wealth across all horses and doesn't hold on to any of his money. Specifically, $b_i$ is the fraction of wealth invested in horse $i$ where $b_i \geq 0$ and $\sum b_i = 1$. If horse $i$ wins, the gambler wins $o_i b_i$; this case occurs with probability $p_i$. The wealth at the end of the race is a random variable which we will attempt to maximize. 

\subsection{Repeated Gambling}

Define $S_n$ as the total growth in the gambler's wealth after $n$ races. We have $$S_n = \prod_{j=1}^n S(X_j) \qquad \text{and} \qquad S(X_j) = b(X_j) o(X_j)$$with $X$ representing the horse that wins (this changes between races). Here, $S(X_i)$ represents the factor by which the gambler's wealth grows. We can define the doubling rate of a race $W$ as $$E(\log S(X)) = \sum_{k=1}^m p_k \log (b_k o_k) = W(b, p)$$

\theorem Let race outcomes $X_1, X_2, X_3, \dots, X_n$ be identically and independently distributed $\sim p(x)$. The wealth of a gambler using betting strategy $b$ grows exponentially at the rate $W(b, p)$ such that $S_n = 2^{n W(b, p)}$.

\begin{proof}
Functions of independent random variables are also independent, so $\log S(X_1), \dots \log S(X_n)$ are i.i.d. From our earlier definition of $S_n$ we have $$\frac{1}{n} \log S_n = \frac{1}{n} \sum \log S(X_i)$$By the weak law of large numbers\footnote{See \texttt{http://mathworld.wolfram.com/WeakLawofLargeNumbers.html} for more information.}, this equates to $E(\log S(X)) = W(b, p)$. So we can conclude that $S_n =  2^{n W(b, p)}$ and the proof is complete. So if to maximize $S_n$, we'll need to maximize $W$. 
\end{proof}
\definition The optimum doubling rate over all choices of $b_i$ is $$W^*(p) = \max_b W(b, p) = \max_{b: \: b_i \geq 0, \: \sum b_i = 1} \sum_i p_i \log b_i o_i$$We must formally maximize $W(b, p)$ such that $\sum b_i = 1$. To do this, we'll apply Lagrange optimization. We have $$J(b) = \sum p_i \log b_i o_i + \lambda \sum b_i$$Taking the partial with respect to $b_i$ and setting it equal to 0, $$\frac{\partial J}{\partial b_i} = \frac{p_i}{b_i} + \lambda$$ where $i \in \{ 1 \dots m \}$. $\sum b_i = 1$ results in $b_i = p_i$. Technically, we'd have to take the second derivative to prove that this is a maximum; this verification is left to the reader. 

\theorem $W^* = \sum p_i \log o_i - H(p)$

\begin{proof}
\begin{align*}
W(b, p) &= \sum p_i \log b_i o_i  \\
&= p_i \log \left( \frac{b_i}{p_i} \times p_i o_i \right) \\
&= \sum p_i \log o_i - H(p) - D(p || b)
\end{align*}
The last term, $D$, is known as relative entropy. It has some of the same properties of entropy, one of them being that $D \geq 0$. So, we have that $W(b, p) \leq \sum p_i \log o_i - H(p)$ with equality when $p = b$. 
\end{proof}

\subsection{Kullback-Liebler Divergence}

The function $D$, known as the relative entropy or Kullback-Liebler Divergence, is a measure of distance\footnote{This isn't technically a measure of distance as it doesn't satisfy the triangle inequality} between two distributions. If $p$ and $q$ are the two distributions, then $D(p || q)$ is a measure of inefficiency of assuming $q$ when the true distribution is $p$. The average code length for distribution $p$ is $H(p)$, but if we were to use the code for $q$ to encode $p$, then $H(p) + D(p||q)$ bits. 

\definition The KL divergence $D(p||q)$ is expressed as $$D(p||q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = E_x \left[ \log \frac{p(x)}{q(x)} \right] $$ where $0 \log 0/q = 0$ and $p \log p / 0 = \infty$. We can then write $I(X; Y) = \sum \sum p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$ which is simplified to $D(p(x, y) || p(x) p(Y))$

\example $p(0) = 1-r, q(0) = 1-s, p(1) = r, q(1) = s$
$$D(p||q) = (1-r) \log \frac{1-r}{1-s} + r \log \frac{r}{s}$$ 
$$D(q||p) = (1-s) \log \frac{1-s}{1-r} + s \log \frac{s}{r}$$

\example Consider a case with two horses where horse 1 wins with probability $p_1$ and horse 2 wins with $p_2$. Assume even odds (2-for-1). (a) What is the optimal bet? (b) Doubling rate? (c) Resulting wealth? 

(a) The optimal bet is according to the probabilities of the horses, (b) The doubling rate is $1-H(p)$, and the resulting wealth (c) is $2^{n(1-H(p))}$

We further have that $W(b, p) = \sum p \log \frac{p_i}{r_i} - \sum p \log \frac{p}{b} = D(p || r) - D(p || b)$. The doubling rate is the difference between the distance of the bookie's estimates from the truth. The gambler only makes money when $b$ is closer than $r$. When the odds are $m$-for-1, we have $$W^*(p) = D(p || 1/m)= \log m - H(p)$$and $W^*(p) + H(p) = \log m$.
 
 \example Three horses run a race. A gambler offers 3-for-1 odds on each horse. Fair odds under the assumption that all horses are equally likely to win. $p = (1/2, 1/4, 1/4)$. (a) Expected wealth, (b) $b^*$, (c) $W^*$
 
(a) We have that $W(b) = \sum p_i \log b_i o_i = \sum p_i \log 3b$ since $o_i = 1/3$ due to fair odds. Therefore, $W(b) = \sum p_i \log 3 + \sum p_i \log b_i = \log 3 + \sum p_i \log b_i$. (b) $b^* = p = (1/2, 1/4/, 1/4)$ and (c) $W^* = W(b^*) - \log 3 - 3/2$. Note that we can solve (c) with the identity discussed above. 

\subsection{The Value of Side Information}

One measure is the increase in the doubling rate based on the information. We'll connect this increase with mutual information (as we connected $W^*$ with KL divergence and entropy before). Define $X \in \{1, 2, \dots m \}$ as the horse betting space, $p(x)$ as the probabilities associated with $1 \rightarrow m$, $o(x)$ for 1 odds, and $y$ as the side information. Furthermore, we have $\sum_x b(x|y)$ as the conditional betting depending on side information $y$ and $b(x|y)$ as the proportion of wealth bet on horse $x$ when $y$ is observed. Based on these definitions, we have $$W^*(X) = \max_{b(x)} \sum_x p(x) \log b(x) o(x)$$and given our side information, $$W^*(X|Y) = \max_{b(x|y)} \sum_{x, y} p(x, y) \log b(x|y) o(x)$$so we have $$\Delta W = W^*(X|Y) - W(X)$$

\theorem The increase doubling rate $\Delta W$ due to side information $Y$ for a horse race $X$ is $\Delta W = I(X; Y).$

\begin{proof}
We have that $b^*(x|y) = p(x|y)$. Since $W^*(X|Y) = \max_{b(x|y)} E(\log S)$\footnote{$S$ was defined earlier as the aggregate wealth}. This equates to $\max_{b(x|y)} \sum p(x, y) \log [o(x) b(x|y)]$. So, we have that $$W^*(X|Y) = \sum p(x,y) \log [p(x) p(x|y)] = \sum p(x) \log o(x) - H(X|Y)$$Without side information $W^* = \sum p(x) \log o(x) - H(X)$, so we have $\Delta W = \sum p(x) \log o(x) - H(X|Y) - [\sum p(x) \log o(x) - H(X)]$. Finally, we have $\Delta W = H(X) - H(X|Y) = I(X;Y)$.
\end{proof}

\example Given a three horse race $p = (1/2, 1/4, 1/4)$ with odds with respect to the false distribution $r_1, r_2, r_3 = (1/4, 1/4, 1/2)$ and $o_1, o_2, o_3 = (4, 4, 2)$\footnote{This is because $o_i = 1/r_i$ when determining the odds given the false distribution. ``Fair odds'' are defined such that $\sum 1/o_i = 1$}. Find (a) the entropy of the race and (b) $(b_1, b_2, b_3)$ such that compounded wealth $\rightarrow \infty$. 

The entropy of the race is easily calculated as $3/2$. It's intuitive that $b_i = o_i p_i$ so we have $(2, 1, 1/2)$, which we re-normalize to $(4/7, 2/7, 1/7)$. Our final $W = \sum p_i \log b_i o_i$. 

\example Let the distribution be $(p_1, p_2, p_3)$ with odds $o = (1, 1, 1)$ and wealth proportions $b = (b_1, b_2, b_3)$. $S_n \rightarrow 0$ exponentially. (a) Find the exponent, (b) $b^*$, and (c) What $p$ causes $S_n \rightarrow 0$ at the fastest rate. 

We always have that $b_i = \frac{p_i o_i}{\sum_i b_i}$, so we can write $b^* = p$. Furthermore, the exponent is simply the doubling rate $W = \sum p_i \log b_i o_i$, and the $P$ that causes $S_n \rightarrow 0$ most quickly is the one that maximizes $H(p)$ or $p = (1/3, 1/3, 1/3)$. 

\example Now assume you have the most common form of side information, the past performance of the horses. If the results of each successive horse race are independent, then this additional information will not reveal anything new about the race. Suppose instead that the races are dependent. How might we calculate $W^*(X_k | X_{k-1}, X_{k-2},..., X_1)$?

\begin{proof}
At the end of of $n$ races, $$S_n = \prod_i S(X_i)$$
Therefore,
\begin{align*}
\frac{1}{n} \mathrm{E}\left[\log{S_n}\right] &= \frac{1}{n} \sum_{i=1}^{n} \mathrm{E}\left[\log{S(X_i)}\right]\\
							        &= \frac{1}{n} \sum_{i=1}^{n} \left[\log{m} - H(X_i | ...)\right]\\
							  W^*&=  \log{m} - \frac{H(X_1, X_2,...,X_n)}{n}
\end{align*}
The term $\frac{H(X_1, X_2,...,X_n)}{n}$ can be thought of as the average entropy across all the races. 
\end{proof}

\newpage
\section*{Appendix A---Quotes}
\begin{itemize}
\item ``I have a problem. It's called gambling.'' (Dr. Aiyer)
\item ``What's $E$? Entropy?'' (David Zhu)
\item ``So is it the strong law of weak numbers?'' (Jerry Chen)
\item ``It's like a half life... but it's a double life'' (Steven Cao)
\item ``Isn't this just Lagrange?''  \\ 10 minutes later... \\ ``Wait, how do you do Lagrange again?'' (Swapnil Garg)
\end{itemize}
\end{document}
